{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic modelling Quran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents  40485\n",
      "<class 'list'>\n",
      "['', 'Sura I.', 'Fatiha, or the Opening Chapter.', '1. In the name of God, Most Gracious,', 'Most Merciful.', '2. Praise be to God,', 'The Cherisher and Sustainer of the Worlds;', '3. Most Gracious, Most Merciful;', '4. Master of the Day of Judgment.', '5. Thee do we worship,']\n"
     ]
    }
   ],
   "source": [
    "# READ TEXT and CREATE DOCUMENT - sentences in list\n",
    "# Uses the translation from Yousaf Ali\n",
    "# Available at\n",
    "# https://www.sacred-texts.com/isl/yaq/index.htm\n",
    "fname = \"//Quran/YousafAliQ.txt\"\n",
    "fobj = open(fname, \"r\")\n",
    "doc_complete = [\"\"]\n",
    "\n",
    "for l in fobj:\n",
    "    # print(l)\n",
    "    l = l.strip()\n",
    "    # if not l:\n",
    "    if not l:  # because am and pm is used\n",
    "        continue\n",
    "        print(\"FOUND EMPTY or Hanging String\")\n",
    "    else:\n",
    "        #print(l)\n",
    "        if len(l)>1:\n",
    "            doc_complete.append(l.strip())\n",
    "        \n",
    "\n",
    "print(\"Total documents \", len(doc_complete))\n",
    "print (type(doc_complete))\n",
    "print (doc_complete[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40485\n",
      "2. Praise be to God,\n"
     ]
    }
   ],
   "source": [
    "# change to data frame for better handling\n",
    "import pandas as pd\n",
    "docs = pd.DataFrame(doc_complete)\n",
    "#print(docs[:10])\n",
    "print(len(docs))\n",
    "print(docs[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\usman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fix imports\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "# clean and then stem\n",
    "def lemmatize_stemming(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['2.', 'Praise', 'be', 'to', 'God,']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['praise', 'god']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = docs[0][5]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                []\n",
       "1                            [sura]\n",
       "2           [fatiha, open, chapter]\n",
       "3                   [god, gracious]\n",
       "4                        [merciful]\n",
       "5                     [praise, god]\n",
       "6    [cherisher, sustainer, worlds]\n",
       "7              [gracious, merciful]\n",
       "8           [master, day, judgment]\n",
       "9                   [thee, worship]\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do preprocess on frame\n",
    "pro_docs = docs[0].map(preprocess)\n",
    "pro_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary\n",
    "qd = gensim.corpora.Dictionary(pro_docs)\n",
    "#count = 0\n",
    "#for k, v in dictionary.iteritems():\n",
    "    #print(k, v)\n",
    "    #count += 1\n",
    "    #if count > 10:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "qd.filter_extremes(no_below=15, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 1), (7, 1)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagOfwords = [dictionary.doc2bow(doc) for doc in pro_docs]\n",
    "bagOfwords[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 4 (\"god\") appears 1 time.\n",
      "Word 7 (\"praise\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# explanation\n",
    "chwo = bagOfwords[5]\n",
    "for i in range(len(chwo)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(chwo[i][0], dictionary[chwo[i][0]], chwo[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[(0, 1.0)]\n",
      "[(1, 0.6486035766533425), (2, 0.6486035766533425), (3, 0.3982798020299091)]\n",
      "[(4, 0.4237363322512761), (5, 0.9057855820944801)]\n",
      "[(6, 1.0)]\n",
      "[(4, 0.39497463122018167), (7, 0.9186920271192526)]\n"
     ]
    }
   ],
   "source": [
    "# create the tfidf\n",
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bagOfwords)\n",
    "corpus_tfidf = tfidf[bagOfwords]\n",
    "from pprint import pprint\n",
    "count = 0\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    count+=1\n",
    "    if count>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "lda_model = gensim.models.LdaMulticore(bagOfwords, num_topics=5, id2word=dictionary, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      " 0.190*\"god\" + 0.044*\"say\" + 0.034*\"believe\" + 0.026*\"section\" + 0.023*\"know\" + 0.019*\"turn\" + 0.016*\"fear\" + 0.012*\"gracious\" + 0.011*\"away\" + 0.011*\"truly\"\n",
      "Topic: 1 \n",
      " 0.030*\"people\" + 0.021*\"men\" + 0.017*\"create\" + 0.015*\"follow\" + 0.014*\"unbelievers\" + 0.014*\"power\" + 0.013*\"let\" + 0.013*\"hath\" + 0.012*\"like\" + 0.012*\"man\"\n",
      "Topic: 2 \n",
      " 0.040*\"thee\" + 0.034*\"send\" + 0.029*\"sign\" + 0.023*\"reject\" + 0.020*\"good\" + 0.018*\"faith\" + 0.015*\"verily\" + 0.014*\"knowledge\" + 0.013*\"deeds\" + 0.012*\"clear\"\n",
      "Topic: 3 \n",
      " 0.051*\"thou\" + 0.039*\"day\" + 0.029*\"shall\" + 0.019*\"things\" + 0.017*\"earth\" + 0.017*\"penalty\" + 0.016*\"bring\" + 0.012*\"reward\" + 0.011*\"forgive\" + 0.010*\"forth\"\n",
      "Topic: 4 \n",
      " 0.070*\"lord\" + 0.035*\"thy\" + 0.031*\"come\" + 0.021*\"truth\" + 0.020*\"give\" + 0.018*\"evil\" + 0.017*\"apostle\" + 0.015*\"merciful\" + 0.014*\"wrong\" + 0.014*\"heavens\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\n {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
